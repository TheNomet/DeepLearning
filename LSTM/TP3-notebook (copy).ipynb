{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1 style=\"text-align:center\">Deep Learning   </h1>\n",
    "<h1 style=\"text-align:center\"> Lab Session 3 - 3 Hours </h1>\n",
    "<h1 style=\"text-align:center\">Long Short Term Memory (LSTM) for Language Modeling</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b> Student 1:</b> Hanna Johansson \n",
    "<b> Student 2:</b> Matteo Fiore\n",
    " \n",
    " \n",
    "In this Lab Session,  you will build and train a Recurrent Neural Network, based on Long Short-Term Memory (LSTM) units for next word prediction task. \n",
    "\n",
    "Answers and experiments should be made by groups of one or two students. Each group should fill and run appropriate notebook cells. \n",
    "Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an pdf document using print as PDF (Ctrl+P). Do not forget to run all your cells before generating your final report and do not forget to include the names of all participants in the group. The lab session should be completed by June 9th 2017.\n",
    "\n",
    "Send you pdf file to benoit.huet@eurecom.fr and olfa.ben-ahmed@eurecom.fr using **[DeepLearning_lab3]** as Subject of your email."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#  Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You will train a LSTM to predict the next word using a sample short story. The LSTM will learn to predict the next item of a sentence from the 3 previous items (given as input). Ponctuation marks are considered as dictionnary items so they can be predicted too. Figure 1 shows the LSTM and the process of next word prediction. \n",
    "\n",
    "<img src=\"lstm.png\" height=\"370\" width=\"370\"> \n",
    "\n",
    "\n",
    "Each word (and punctuation) from text sentences is encoded by a unique integer. The integer value corresponds to the index of the corresponding word (or punctuation mark) in the dictionnary. The network output is a one-hot-vector indicating the index of the predicted word in the reversed dictionnary (Section 1.2). For example if the prediction is 86, the predicted word will be \"company\". \n",
    "\n",
    "\n",
    "\n",
    "You will use a sample short story from Aesopâ€™s Fables (http://www.taleswithmorals.com/) to train your model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<font size=\"3\" face=\"verdana\" > <i> \"There was once a young Shepherd Boy who tended his sheep at the foot of a mountain near a dark forest.\n",
    "\n",
    "It was rather lonely for him all day, so he thought upon a plan by which he could get a little company and some excitement.\n",
    "He rushed down towards the village calling out \"Wolf, Wolf,\" and the villagers came out to meet him, and some of them stopped with him for a considerable time.\n",
    "This pleased the boy so much that a few days afterwards he tried the same trick, and again the villagers came to his help.\n",
    "But shortly after this a Wolf actually did come out from the forest, and began to worry the sheep, and the boy of course cried out \"Wolf, Wolf,\" still louder than before.\n",
    "But this time the villagers, who had been fooled twice before, thought the boy was again deceiving them, and nobody stirred to come to his help.\n",
    "So the Wolf made a good meal off the boy's flock, and when the boy complained, the wise man of the village said:\n",
    "\"A liar will not be believed, even when he speaks the truth.\"  \"</i> </font>.    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Start by loading the necessary libraries and resetting the default computational graph. For more details about the rnn packages, we suggest you to take a look at https://www.tensorflow.org/api_guides/python/contrib.rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections # used to build the dictionary\n",
    "import random\n",
    "import time\n",
    "import pickle # may be used to save your model \n",
    "import matplotlib.pyplot as plt\n",
    "#Import Tensorflow and rnn\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn  \n",
    "\n",
    "# Target log path\n",
    "logs_path = 'lstm_words'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Next-word prediction task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 1: Data  preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.1. Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Load and split the text of our story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['there' 'was' 'once' 'a' 'young' 'shepherd' 'boy' 'who' 'tended' 'his'\n",
      " 'sheep' 'at' 'the' 'foot' 'of' 'a' 'mountain' 'near' 'a' 'dark' 'forest'\n",
      " '.' 'it' 'was' 'rather' 'lonely' 'for' 'him' 'all' 'day' ',' 'so' 'he'\n",
      " 'thought' 'upon' 'a' 'plan' 'by' 'which' 'he' 'could' 'get' 'a' 'little'\n",
      " 'company' 'and' 'some' 'excitement' '.' 'he' 'rushed' 'down' 'towards'\n",
      " 'the' 'village' 'calling' 'out' 'wolf' ',' 'wolf' ',' 'and' 'the'\n",
      " 'villagers' 'came' 'out' 'to' 'meet' 'him' ',' 'and' 'some' 'of' 'them'\n",
      " 'stopped' 'with' 'him' 'for' 'a' 'considerable' 'time' '.' 'this'\n",
      " 'pleased' 'the' 'boy' 'so' 'much' 'that' 'a' 'few' 'days' 'afterwards'\n",
      " 'he' 'tried' 'the' 'same' 'trick' ',' 'and' 'again' 'the' 'villagers'\n",
      " 'came' 'to' 'his' 'help' '.' 'but' 'shortly' 'after' 'this' 'a' 'wolf'\n",
      " 'actually' 'did' 'come' 'out' 'from' 'the' 'forest' ',' 'and' 'began' 'to'\n",
      " 'worry' 'the' 'sheep,' 'and' 'the' 'boy' 'of' 'course' 'cried' 'out'\n",
      " 'wolf' ',' 'wolf' ',' 'still' 'louder' 'than' 'before' '.' 'but' 'this'\n",
      " 'time' 'the' 'villagers' ',' 'who' 'had' 'been' 'fooled' 'twice' 'before'\n",
      " ',' 'thought' 'the' 'boy' 'was' 'again' 'deceiving' 'them' ',' 'and'\n",
      " 'nobody' 'stirred' 'to' 'come' 'to' 'his' 'help' '.' 'so' 'the' 'wolf'\n",
      " 'made' 'a' 'good' 'meal' 'off' 'the' \"boy's\" 'flock' ',' 'and' 'when'\n",
      " 'the' 'boy' 'complained' ',' 'the' 'wise' 'man' 'of' 'the' 'village'\n",
      " 'said' ':' 'a' 'liar' 'will' 'not' 'be' 'believed' ',' 'even' 'when' 'he'\n",
      " 'speaks' 'the' 'truth' '.']\n",
      "Loaded training data...\n",
      "214\n"
     ]
    }
   ],
   "source": [
    "def load_data(filename):\n",
    "    with open(filename) as f:\n",
    "        data = f.readlines()\n",
    "    data = [x.strip().lower() for x in data]\n",
    "    data = [data[i].split() for i in range(len(data))]\n",
    "    data = np.array(data)\n",
    "    data = np.reshape(data, [-1, ])\n",
    "    print(data)\n",
    "    return data\n",
    "\n",
    "#Run the cell \n",
    "train_file ='data/story.txt'\n",
    "train_data = load_data(train_file)\n",
    "print(\"Loaded training data...\")\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.2.Symbols encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The LSTM input's can only be numbers. A way to convert words (symbols or any items) to numbers is to assign a unique integer to each word. This process is often based on frequency of occurrence for efficient coding purpose.\n",
    "\n",
    "Here, we define a function to build an indexed word dictionary (word->number). The \"build_vocabulary\" function builds both:\n",
    "\n",
    "- Dictionary : used for encoding words to numbers for the LSTM inputs \n",
    "- Reverted dictionnary : used for decoding the outputs of the LSTM into words (and punctuation).\n",
    "\n",
    "For example, in the story above, we have **113** individual words. The \"build_vocabulary\" function builds a dictionary with the following entries ['the': 0], [',': 1], ['company': 85],...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_vocabulary(words):\n",
    "    count = collections.Counter(words).most_common()\n",
    "    dic= dict()\n",
    "    for word, _ in count:\n",
    "        dic[word] = len(dic)\n",
    "    reverse_dic= dict(zip(dic.values(), dic.keys()))\n",
    "    return dic, reverse_dic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Run the cell below to display the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary size (Vocabulary size) =  113\n",
      "\n",
      "\n",
      "Dictionary : \n",
      "\n",
      "{'company': 32, 'louder': 95, 'but': 21, 'young': 34, 'shortly': 35, 'it': 36, 'this': 11, 'did': 37, 'dark': 33, 'the': 0, 'nobody': 38, 'rushed': 40, 'lonely': 42, 'meal': 43, 'not': 44, 'tended': 45, 'towards': 46, 'could': 48, 'stopped': 49, 'again': 18, 'meet': 51, 'rather': 52, 'believed': 53, 'began': 54, 'once': 55, 'had': 56, 'than': 57, 'liar': 41, 'course': 58, 'wolf': 5, 'trick': 59, 'will': 60, 'sheep,': 61, 'that': 62, 'there': 101, 'who': 17, 'he': 6, 'out': 9, 'calling': 64, 'flock': 65, 'get': 94, 'forest': 19, 'at': 67, 'pleased': 68, 'excitement': 69, 'after': 70, 'deceiving': 82, 'came': 20, 'fooled': 72, 'help': 22, 'day': 73, 'when': 23, ':': 74, 'few': 75, 'good': 76, 'so': 13, 'actually': 77, \"boy's\": 79, 'before': 24, 'which': 71, 'with': 80, 'complained': 81, 'to': 7, 'him': 12, 'wise': 87, 'thought': 25, 'cried': 83, 'worry': 63, 'was': 14, 'time': 28, 'still': 85, 'tried': 86, 'down': 107, 'little': 88, 'all': 89, 'made': 90, 'villagers': 15, 'days': 109, 'by': 91, 'upon': 92, 'truth': 93, 'same': 66, 'come': 26, ',': 1, 'afterwards': 84, 'be': 96, 'a': 2, 'some': 27, 'mountain': 98, 'off': 97, 'man': 39, '.': 4, 'and': 3, 'sheep': 99, 'twice': 100, 'stirred': 50, 'from': 102, 'plan': 103, 'considerable': 104, 'been': 105, 'village': 29, 'foot': 106, 'speaks': 47, 'said': 108, 'much': 78, 'for': 30, 'near': 110, 'his': 16, 'them': 31, 'even': 111, 'of': 10, 'boy': 8, 'shepherd': 112}\n",
      "\n",
      "\n",
      "Reversed Dictionary : \n",
      "\n",
      "{0: 'the', 1: ',', 2: 'a', 3: 'and', 4: '.', 5: 'wolf', 6: 'he', 7: 'to', 8: 'boy', 9: 'out', 10: 'of', 11: 'this', 12: 'him', 13: 'so', 14: 'was', 15: 'villagers', 16: 'his', 17: 'who', 18: 'again', 19: 'forest', 20: 'came', 21: 'but', 22: 'help', 23: 'when', 24: 'before', 25: 'thought', 26: 'come', 27: 'some', 28: 'time', 29: 'village', 30: 'for', 31: 'them', 32: 'company', 33: 'dark', 34: 'young', 35: 'shortly', 36: 'it', 37: 'did', 38: 'nobody', 39: 'man', 40: 'rushed', 41: 'liar', 42: 'lonely', 43: 'meal', 44: 'not', 45: 'tended', 46: 'towards', 47: 'speaks', 48: 'could', 49: 'stopped', 50: 'stirred', 51: 'meet', 52: 'rather', 53: 'believed', 54: 'began', 55: 'once', 56: 'had', 57: 'than', 58: 'course', 59: 'trick', 60: 'will', 61: 'sheep,', 62: 'that', 63: 'worry', 64: 'calling', 65: 'flock', 66: 'same', 67: 'at', 68: 'pleased', 69: 'excitement', 70: 'after', 71: 'which', 72: 'fooled', 73: 'day', 74: ':', 75: 'few', 76: 'good', 77: 'actually', 78: 'much', 79: \"boy's\", 80: 'with', 81: 'complained', 82: 'deceiving', 83: 'cried', 84: 'afterwards', 85: 'still', 86: 'tried', 87: 'wise', 88: 'little', 89: 'all', 90: 'made', 91: 'by', 92: 'upon', 93: 'truth', 94: 'get', 95: 'louder', 96: 'be', 97: 'off', 98: 'mountain', 99: 'sheep', 100: 'twice', 101: 'there', 102: 'from', 103: 'plan', 104: 'considerable', 105: 'been', 106: 'foot', 107: 'down', 108: 'said', 109: 'days', 110: 'near', 111: 'even', 112: 'shepherd'}\n"
     ]
    }
   ],
   "source": [
    "dictionary, reverse_dictionary = build_vocabulary(train_data)\n",
    "vocabulary_size= len(dictionary) \n",
    "print(\"Dictionary size (Vocabulary size) = \", vocabulary_size)\n",
    "print(\"\\n\")\n",
    "print(\"Dictionary : \\n\")\n",
    "print(dictionary)\n",
    "print(\"\\n\")\n",
    "print(\"Reversed Dictionary : \\n\" )\n",
    "print(reverse_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 2 : LSTM Model in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Since you have defined how the data will be modeled, you are now to develop an LSTM model to predict the word of following a sequence of 3 words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Define a 2-layers LSTM model.  \n",
    "\n",
    "For this use the following classes from the tensorflow.contrib library:\n",
    "\n",
    "- rnn.BasicLSTMCell(number of hidden units) \n",
    "- rnn.static_rnn(rnn_cell, data, dtype=tf.float32)\n",
    "- rnn.MultiRNNCell(,)\n",
    "\n",
    "\n",
    "You may need some tensorflow functions (https://www.tensorflow.org/api_docs/python/tf/) :\n",
    "- tf.split\n",
    "- tf.reshape \n",
    "- ...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<div style=\"background-color:#327191; vertical-align: middle; padding:5px 0px 10px 10px;\">\n",
    "    <h2><font color='white'>What we did</font></h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<div style=\"background-color:#d9e6fc; padding:10px 15px 10px 15px;\">\n",
    "We decided to merge part 2 (LSTM Model in TensorFlow), part 3 (LSTM Training) and part 4 (Test your model) and to create one class for the model, which is covering model definition, training and testing. There are several reasons for this choice of implementation, listed below:\n",
    "<ul>\n",
    "<li>When it comes to changing parameters it is easier to have one class, which we can have several instances of with different parameters, rather than copying and pasting the same code multiple times. \n",
    "<li>It is easier to have one class containing the implementation of the different functions, than to call each function separately.\n",
    "<li>Overall, we think it is more clear to use a class in this way and it gives a better overview of the implementation.\n",
    "</ul>\n",
    "<br>\n",
    "We have commented within the class to indicate which parts of code belongs to which section in the original guidelines of the lab.\n",
    "<br>\n",
    "<h4>Regarding the testing..</h4><br>\n",
    "Initially we were given a test function that we were supposed to use for the prediction of one word and later on five sentences. Instead of using this function as it was originally we decided to modify it. We did the modifications to obtain a unique function that the user can use to directly generate a given number of words or sentences.The function is defined within the class, but the actual testing of the model is still done in part 4.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    \n",
    "    #=====================================================\n",
    "    #    Initialization, default parameters\n",
    "    #=====================================================\n",
    "    def __init__(self,dictionary, reverse_dictionary,logs_path,train_data,model='my_model',\n",
    "                 learning_rate = 0.001,epochs = 50000,display_step = 1000,n_input = 3,n_hidden = 64\n",
    "                 ):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.display_step = display_step\n",
    "        self.n_input = n_input\n",
    "        self.n_hidden = n_hidden\n",
    "        self.vocabulary_size = len(dictionary) \n",
    "        self.dictionary = dictionary\n",
    "        self.reverse_dictionary = reverse_dictionary\n",
    "        self.logs_path = logs_path\n",
    "        self.train_data = train_data\n",
    "        self.model = 'lstm_model/'+model\n",
    "        \n",
    "    #=====================================================\n",
    "    #    2. Model definition\n",
    "    #=====================================================\n",
    "    def lstm_model(self, x, w, b):\n",
    "        \"\"\"defines the model that we are going to use\"\"\"\n",
    "\n",
    "        x = tf.reshape(x, [-1, self.n_input])\n",
    "        # Generate a n_input-element sequence of inputs\n",
    "        # (eg. [had] [a] [general] -> [20] [6] [33])\n",
    "        x = tf.split(x,self.n_input,1)\n",
    "\n",
    "        # 1-layer LSTM with n_hidden units.\n",
    "        rnn_cell = rnn.BasicLSTMCell(self.n_hidden)\n",
    "\n",
    "        # 2-layer LSTM with n_hidden units.\n",
    "        rnn_cell2 = rnn.BasicLSTMCell(self.n_hidden)\n",
    "\n",
    "        # multi-rnn from the two basic lstm cells\n",
    "        multi_rnn = rnn.MultiRNNCell([rnn_cell, rnn_cell2])\n",
    "\n",
    "        # generate prediction\n",
    "        outputs, states = rnn.static_rnn(multi_rnn, x, dtype=tf.float32)\n",
    "\n",
    "        # there are n_input outputs but\n",
    "        # we only want the last output\n",
    "        return tf.matmul(outputs[-1], w['out']) + b['out']\n",
    "    \n",
    "    #=====================================================\n",
    "    #    3. LSTM Training\n",
    "    #=====================================================\n",
    "    def train(self):\n",
    "        \"\"\"defines the training phase of the model\"\"\"\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        #=====================================================\n",
    "        #    Training parameters and constants\n",
    "        #=====================================================\n",
    "        # tf Graph input\n",
    "        self.x = tf.placeholder(\"float\", [None, self.n_input, 1], name='InputData')\n",
    "        self.y = tf.placeholder(\"float\", [None, self.vocabulary_size], name='Labels')\n",
    "\n",
    "        # LSTM  weights and biases\n",
    "        weights = {'out': tf.Variable(tf.random_normal([self.n_hidden, self.vocabulary_size]))}\n",
    "        biases = {'out': tf.Variable(tf.random_normal([self.vocabulary_size])) }\n",
    "\n",
    "        #build the model\n",
    "        with tf.name_scope('Model'):\n",
    "            self.pred = self.lstm_model(self.x, weights, biases)\n",
    "        \n",
    "        #=====================================================\n",
    "        #    Define Loss/Cost and optimizer\n",
    "        #=====================================================\n",
    "        with tf.name_scope('Loss'):\n",
    "            # Loss and optimizer\n",
    "            cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.pred, labels=self.y))\n",
    "        with tf.name_scope('RMSPOpt'):    \n",
    "            #use RMSProp Optimizer\n",
    "            optimizer = tf.train.RMSPropOptimizer(learning_rate=self.learning_rate).minimize(cost)\n",
    "\n",
    "        # Model evaluation\n",
    "        correct_pred = tf.equal(tf.argmax(self.pred,1), tf.argmax(self.y,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        \n",
    "        #=====================================================\n",
    "        #    Initialize variables and summary\n",
    "        #=====================================================\n",
    "        # Initializing the variables\n",
    "        start_time = time.time()\n",
    "        self.init = tf.global_variables_initializer()\n",
    "\n",
    "        # Create a summary to monitor cost and accuracy tensor\n",
    "        tf.summary.scalar(\"Loss\", cost)\n",
    "        tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "        merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "        # Initialize the saver\n",
    "        self.model_saver = tf.train.Saver()\n",
    "\n",
    "        #=====================================================\n",
    "        #    Training\n",
    "        #=====================================================\n",
    "        print(\"Start Training\")\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(self.init)\n",
    "            # op to write logs to Tensorboard\n",
    "            summary_writer = tf.summary.FileWriter(self.logs_path, graph=tf.get_default_graph())\n",
    "            # Training cycle\n",
    "            for epoch in range(self.epochs):\n",
    "                avg_cost = 0.\n",
    "                offset = epoch % (len(self.train_data)-self.n_input)\n",
    "                # 3 words are taken from the training data, encoded to integer to form the input vector\n",
    "                symbols_in_keys = np.array([ [dictionary[ str(self.train_data[i])]] \n",
    "                                            for i in range(offset, offset+self.n_input) ])\n",
    "                symbols_in_keys = symbols_in_keys.reshape(-1,self.n_input,1)\n",
    "\n",
    "                # creation of the one-hot vector for training labels\n",
    "                symbols_out_onehot = np.array(np.zeros([self.vocabulary_size], dtype=float))\n",
    "                # putting to one the cell of the prediction\n",
    "                symbols_out_onehot[dictionary[str(train_data[offset+self.n_input])]] = 1.0\n",
    "                # reshaping \n",
    "                symbols_out_onehot = symbols_out_onehot.reshape(-1,self.vocabulary_size)\n",
    "\n",
    "                # running the session\n",
    "                _, acc, loss, summary, onehot_pred = sess.run([optimizer, accuracy, cost,\n",
    "                                merged_summary_op, self.pred], \n",
    "                                feed_dict={self.x: symbols_in_keys, self.y: symbols_out_onehot})\n",
    "                \n",
    "                summary_writer.add_summary(summary, epoch)\n",
    "                # Display logs per epoch step\n",
    "                if (epoch+1) % self.display_step == 0:\n",
    "                    print(\"Epoch: \", '%02d' % (epoch+1))\n",
    "                    print(\"\\t\\t=====> Loss=\", \"{:.9f}\".format(loss))\n",
    "                    print(\"\\t\\t=====> Accuracy=\", \"{:.9f}\".format(acc))\n",
    "\n",
    "            # Print\n",
    "            print(\"End Of training Finished!\")\n",
    "            print(\"time: \",time.time() - start_time)\n",
    "            print(\"For tensorboard visualisation run on command line.\")\n",
    "            print(\"\\ttensorboard --logdir=%s\" % (self.logs_path))\n",
    "            print(\"and point your web browser to the returned link\")\n",
    "            \n",
    "            self.model_saver.save(sess, self.model)\n",
    "            print(\"Model saved\")\n",
    "            \n",
    "    #=====================================================\n",
    "    #    4. Testing the model\n",
    "    #=====================================================     \n",
    "    def test(self, sentences, number_of_sentences=-1, number_of_words=-1):\n",
    "        \"\"\"defines the testing function\n",
    "            Parameters:\n",
    "             - sentences: an array containing starting sentences; if the starting sentences\n",
    "                          contains more words than expected no prediction will be done\n",
    "             - number_of_sentences: if set to -1 no sentence will be created\n",
    "             - number_of_words: taken into account when number_of_sentences set to -1\n",
    "                                indicate how many words we want to predict\n",
    "            If both number_of_sentences and number_of_words are -1, the default behaviour is setting \n",
    "            number_of_sentences to 1\n",
    "        \"\"\"\n",
    "        if number_of_sentences != -1:\n",
    "            number_of_whatever = number_of_sentences  \n",
    "            number_of_words = -1\n",
    "        elif number_of_words != -1:\n",
    "            number_of_whatever = number_of_words\n",
    "            number_of_sentences = -1\n",
    "        else:\n",
    "            number_of_whatever=1\n",
    "            number_of_sentences=1\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            # Initialize variables\n",
    "\n",
    "            sess.run(self.init)\n",
    "            self.model_saver.restore(sess, self.model)\n",
    "#             print('sess ok')\n",
    "            for sentence in sentences:\n",
    "                sentence = sentence.strip()\n",
    "                words = sentence.split(' ')\n",
    "#                 print(words)\n",
    "                s_count=0\n",
    "                w_count=0                \n",
    "                # Checking input dimensions\n",
    "                if len(words) != self.n_input:\n",
    "                    print(\"wrong number of input words\",len(words),self.n_input)\n",
    "                    break\n",
    "                try:\n",
    "                    # Find the key for each word\n",
    "                    symbols_in_keys = [self.dictionary[str(words[i])] for i in range(len(words))]\n",
    "                    print('\\n'+sentence,end=' ')\n",
    "                    # Iterate until the number of sentences/words defined by the user is reached\n",
    "                    while(s_count<number_of_whatever):\n",
    "                        keys = np.reshape(np.array(symbols_in_keys), [-1, self.n_input, 1])\n",
    "                        onehot_pred = sess.run(self.pred, feed_dict={self.x: keys})\n",
    "                        onehot_pred_index = int(tf.argmax(onehot_pred, 1).eval())\n",
    "                        prediction = self.reverse_dictionary[onehot_pred_index]\n",
    "                        sentence = \"%s %s\" % (sentence,prediction)\n",
    "                        print(prediction,end=' ')\n",
    "                        # If number of words was defined by the user -> increment the count\n",
    "                        if number_of_words != -1:\n",
    "                            s_count += 1\n",
    "                        #If number of sentences was defined by the user and a '.' is predicted -> increment\n",
    "                        if prediction == '.' and number_of_sentences != -1:\n",
    "                            s_count += 1\n",
    "                        symbols_in_keys = symbols_in_keys[1:]\n",
    "                        symbols_in_keys.append(onehot_pred_index)\n",
    "                # Catch errors\n",
    "                except:\n",
    "                    print(\"Word not in dictionary\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 3 : LSTM Training  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In the Training process, at each epoch, 3 words are taken from the training data, encoded to integer to form the input vector. The training labels are one-hot vector encoding the word that comes after the 3 inputs words. Display the loss and the training accuracy every 1000 iteration. Save the model at the end of training in the **lstm_model** folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "Epoch:  1000\n",
      "\t\t=====> Loss= 2.589306116\n",
      "\t\t=====> Accuracy= 0.000000000\n",
      "Epoch:  2000\n",
      "\t\t=====> Loss= 2.581354618\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "Epoch:  3000\n",
      "\t\t=====> Loss= 3.488306761\n",
      "\t\t=====> Accuracy= 0.000000000\n",
      "Epoch:  4000\n",
      "\t\t=====> Loss= 5.000435829\n",
      "\t\t=====> Accuracy= 0.000000000\n",
      "Epoch:  5000\n",
      "\t\t=====> Loss= 0.827232361\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "Epoch:  6000\n",
      "\t\t=====> Loss= 1.051124096\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "Epoch:  7000\n",
      "\t\t=====> Loss= 0.192727685\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "Epoch:  8000\n",
      "\t\t=====> Loss= 0.764243901\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "Epoch:  9000\n",
      "\t\t=====> Loss= 0.194511652\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "Epoch:  10000\n",
      "\t\t=====> Loss= 0.634211838\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "Epoch:  11000\n",
      "\t\t=====> Loss= 0.437799424\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "Epoch:  12000\n",
      "\t\t=====> Loss= 0.037846763\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "Epoch:  13000\n",
      "\t\t=====> Loss= 0.109591953\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "Epoch:  14000\n",
      "\t\t=====> Loss= 0.053574841\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "Epoch:  15000\n",
      "\t\t=====> Loss= 0.039812192\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "Epoch:  16000\n",
      "\t\t=====> Loss= 0.072560981\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "Epoch:  17000\n",
      "\t\t=====> Loss= 0.108416609\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "Epoch:  18000\n",
      "\t\t=====> Loss= 0.137103438\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "Epoch:  19000\n",
      "\t\t=====> Loss= 0.419142425\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "Epoch:  20000\n",
      "\t\t=====> Loss= 0.067336783\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "Epoch:  21000\n",
      "\t\t=====> Loss= 0.028290302\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "Epoch:  22000\n",
      "\t\t=====> Loss= 0.007832766\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "Epoch:  23000\n",
      "\t\t=====> Loss= 0.004975675\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "Epoch:  24000\n",
      "\t\t=====> Loss= 0.000565965\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "Epoch:  25000\n",
      "\t\t=====> Loss= 2.123598099\n",
      "\t\t=====> Accuracy= 0.000000000\n",
      "Epoch:  26000\n",
      "\t\t=====> Loss= 0.032564115\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "Epoch:  27000\n",
      "\t\t=====> Loss= 0.323636770\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "Epoch:  28000\n",
      "\t\t=====> Loss= 0.107120156\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "Epoch:  29000\n",
      "\t\t=====> Loss= 0.036330033\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "Epoch:  30000\n",
      "\t\t=====> Loss= 0.283638299\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "Epoch:  31000\n",
      "\t\t=====> Loss= 0.009462034\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "Epoch:  32000\n",
      "\t\t=====> Loss= 0.067831591\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "Epoch:  33000\n",
      "\t\t=====> Loss= 0.073141634\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "Epoch:  34000\n",
      "\t\t=====> Loss= 0.036626689\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "Epoch:  35000\n",
      "\t\t=====> Loss= 0.010681833\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "Epoch:  36000\n",
      "\t\t=====> Loss= 0.051979721\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "Epoch:  37000\n",
      "\t\t=====> Loss= 0.054771602\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "Epoch:  38000\n",
      "\t\t=====> Loss= 0.097427823\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "Epoch:  39000\n",
      "\t\t=====> Loss= 0.001180905\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "Epoch:  40000\n",
      "\t\t=====> Loss= 0.026723480\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "Epoch:  41000\n",
      "\t\t=====> Loss= 0.010561289\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "Epoch:  42000\n",
      "\t\t=====> Loss= 0.021711990\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "Epoch:  43000\n",
      "\t\t=====> Loss= 0.013960919\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "Epoch:  44000\n",
      "\t\t=====> Loss= 0.054227713\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "Epoch:  45000\n",
      "\t\t=====> Loss= 0.000373055\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "Epoch:  46000\n",
      "\t\t=====> Loss= 0.020689273\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "Epoch:  47000\n",
      "\t\t=====> Loss= 0.367639661\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "Epoch:  48000\n",
      "\t\t=====> Loss= 0.016327353\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "Epoch:  49000\n",
      "\t\t=====> Loss= 0.005291623\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "Epoch:  50000\n",
      "\t\t=====> Loss= 0.009979002\n",
      "\t\t=====> Accuracy= 1.000000000\n",
      "End Of training Finished!\n",
      "time:  136.08173274993896\n",
      "For tensorboard visualisation run on command line.\n",
      "\ttensorboard --logdir=lstm_words\n",
      "and point your web browser to the returned link\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "mod = Model(dictionary=dictionary,reverse_dictionary=reverse_dictionary,\n",
    "            logs_path=logs_path,train_data=train_data,\n",
    "            learning_rate = 0.001,epochs = 50000,display_step = 1000,n_input = 3,n_hidden = 64\n",
    "           )\n",
    "mod.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 4 : Test your model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.1. Next word prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Load your model (using the model_saved variable given in the training session) and test the sentences :\n",
    "- 'get a little' \n",
    "- 'nobody tried to'\n",
    "- Try with other sentences using words from the story's vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from lstm_model/my_model\n",
      "\n",
      "a liar come . \n",
      "nobody tried to wolf \n",
      "get a little meal \n",
      "the forest cried out "
     ]
    }
   ],
   "source": [
    "mod.test([\"a liar come\", \"nobody tried to\", \"get a little\", \"the forest cried\"],-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.2. More fun with the Fable Writer !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You will use the RNN/LSTM model learned in the previous question to create a\n",
    "new story/fable.\n",
    "For this you will choose 3 words from the dictionary which will start your\n",
    "story and initialize your network. Using those 3 words the RNN will generate\n",
    "the next word of the story. Using the last 3 words (the newly predicted one\n",
    "and the last 2 from the input) you will use the network to predict the 5\n",
    "word of the story.. and so on until your story is 5 sentence long. \n",
    "Make a point at the end of your story. \n",
    "To implement that, you will use the test function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from lstm_model/my_model\n",
      "\n",
      "a wolf come the boy complained , the wise man of the village said : a liar will not be believed , even when he speaks the truth . a boy , and some the boy was again deceiving them , and nobody stirred to come to his help . so the wolf made a good meal off the boy's flock , and when the boy complained , the wise man of the village said : a liar will not be believed , even when he speaks the truth . a boy , and some the boy was again deceiving them , and nobody stirred to come to his help . so the wolf made a good meal off the boy's flock , and when the boy complained , the wise man of the village said : a liar will not be believed , even when he speaks the truth . "
     ]
    }
   ],
   "source": [
    "mod.test([\"a wolf come\"],5,-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.3. Play with number of inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "The number of input in our example is 3, see what happens when you use other number (1 and 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Your answer goes here"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
